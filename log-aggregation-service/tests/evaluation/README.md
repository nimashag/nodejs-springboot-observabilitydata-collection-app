# Event Type Classifier Evaluation

This directory contains evaluation scripts for comparing different ML approaches to event type classification.

## Overview

The evaluation compares four different approaches:
1. **Rule-based** (Baseline) - Keyword matching
2. **Naive Bayes** - Using `natural` library's BayesClassifier
3. **NLP-based** - Using `node-nlp` for intent recognition
4. **TF-IDF + KNN** - Using TF-IDF vectorization with K-Nearest Neighbors

## Performance Results

### Accuracy Summary

| Classifier | Accuracy | Precision (Avg) | Recall (Avg) | F1-Score (Avg) |
|------------|----------|-----------------|--------------|----------------|
| **Rule-based** | 93.06% | 91.11% | 95.48% | 92.87% |
| **Naive Bayes** | 81.50% | 87.50% | 66.15% | 72.99% |
| **NLP-based** | **97.69%** | **100.00%** | **97.85%** | **98.91%** |
| **TF-IDF KNN** | 53.76% | 13.44% | 25.00% | 17.48% |

*Results based on 70/30 train-test split from aggregated logs (611 samples total: 438 training, 173 test)*

### Per-Class Performance (NLP-based - Best Performer)

| Event Type | Precision | Recall | F1-Score |
|------------|-----------|--------|----------|
| **unknown** | 100.00% | 97.85% | 98.91% |
| **http_request** | 100.00% | 96.92% | 98.44% |
| **database** | 100.00% | 100.00% | 100.00% |
| **authentication** | 100.00% | 100.00% | 100.00% |
| **warning** | N/A | N/A | N/A |
| **error** | N/A | N/A | N/A |

## Running the Evaluation

```bash
npm run evaluate:event-types
```

## What It Does

1. **Loads Dataset**: Extracts templates from aggregated logs using the same parameterization logic as production (`LogParameterizer`)
2. **Ground Truth**: Infers event types from `log.event` using shared utility (`inferEventTypeFromText`) - ensures consistency with rule-based classifier logic
3. **Before Training**: Evaluates each classifier (except rule-based) without training
4. **Training**: Trains ML classifiers on training dataset (70% of data)
5. **After Training**: Re-evaluates all classifiers with trained models on test dataset (30% of data)
6. **Metrics**: Calculates accuracy, precision, recall, F1-score, and confusion matrices
7. **Results**: Saves detailed results to `evaluation-results/` directory (git-ignored)

### Evaluation Methodology

The evaluation simulates production behavior:
- **Templates**: Created using the same `LogParameterizer` utility as production
- **Inference**: Classifiers evaluate on templates (same as production `templateMiner.ts`)
- **Ground Truth**: Derived from `log.event` using shared `inferEventTypeFromText()` utility
- **Consistency**: Both rule-based classifier and ground truth generation use the same keyword matching logic

This ensures that:
- The rule-based classifier provides a meaningful baseline (93.06%, not 100%)
- All classifiers are evaluated on the same input format (templates)
- Results are comparable and reflect real-world production performance

## Output

Results are saved to:
- `evaluation-results/evaluation-{timestamp}.json` - Timestamped results
- `evaluation-results/evaluation-latest.json` - Latest results

Each result file contains:
- Accuracy metrics (before/after training)
- Per-class precision, recall, and F1-scores
- Confusion matrices
- Detailed predictions for each test sample

## Changing the Classifier in Production

To change which classifier is used in the application:

1. Edit `src/config/classifier.config.json`
2. Set `"classifier"` to one of:
   - `"rule-based"`
   - `"naive-bayes"`
   - `"nlp-based"`
   - `"tfidf-knn"`
3. Restart the service

No code changes required!

## Model Comparison & Analysis

### 1. Rule-based (Baseline) - 93.06% Accuracy

**How it works:**
- Simple keyword matching using shared utility (`inferEventTypeFromText`)
- Checks for keywords like "error", "warn", "http.request", "db.", "auth", etc.
- No training required, deterministic results
- Uses the same logic for both classification and ground truth generation (ensuring consistency)

**Strengths:**
- ‚úÖ Fast and lightweight (no model overhead)
- ‚úÖ Interpretable (easy to understand why a classification was made)
- ‚úÖ No training data required
- ‚úÖ Consistent performance (no randomness)
- ‚úÖ Strong baseline performance (93.06% accuracy)
- ‚úÖ Perfect precision for most classes (100% for unknown, http_request, authentication)

**Weaknesses:**
- ‚ùå Limited to predefined patterns
- ‚ùå Cannot learn new patterns from data
- ‚ùå Lower precision for database class (55.56%) due to false positives
- ‚ùå Some confusion between "unknown" and other classes (8 database, 4 error misclassifications)

**Why it performs well:**
The rule-based approach works well because:
- Templates contain clear event indicators (e.g., "http.request.received", "db.connected")
- Shared utility ensures consistent keyword matching logic
- Most event types have distinctive patterns in templates
- The 93.06% baseline provides a meaningful target for ML models to beat

**Best for:** Simple deployments where interpretability and speed are important, or as a reliable baseline for comparison.

---

### 2. Naive Bayes - 81.50% Accuracy

**How it works:**
- Probabilistic classifier based on Bayes' theorem
- Assumes feature independence (words in template)
- Learns word probabilities for each event type from training data (438 samples)

**Strengths:**
- ‚úÖ Good performance improvement after training (53.76% ‚Üí 81.50%)
- ‚úÖ Fast training and inference
- ‚úÖ Works well with moderate-sized datasets
- ‚úÖ Probabilistic output (confidence scores)
- ‚úÖ Handles unseen words gracefully
- ‚úÖ Strong precision (87.50% average)

**Weaknesses:**
- ‚ùå Lower recall (66.15% average) - misses some true positives
- ‚ùå Feature independence assumption is violated (words in logs are contextually dependent)
- ‚ùå Struggles with "unknown" class (65.59% recall) - classifies many as other types
- ‚ùå Some confusion between similar classes

**Why it performs moderately:**
1. **Improved with larger dataset**: 438 training samples (vs 127 previously) helps generalization
2. **Better class balance**: More diverse training data reduces overfitting
3. **Feature independence limitation**: Still struggles with contextual dependencies in log templates
4. **Class imbalance**: "Unknown" class still dominates, affecting recall

**Best for:** Balanced datasets with clear word-class associations. Performs well but still below NLP-based approach.

---

### 3. NLP-based - 97.69% Accuracy ‚≠ê **BEST PERFORMER**

**How it works:**
- Uses `node-nlp` library with neural network-based intent recognition
- Learns semantic patterns and context from training data (438 samples)
- Handles variations in phrasing and format

**Strengths:**
- ‚úÖ **Highest accuracy** (97.69%) - 4.6% improvement over rule-based baseline
- ‚úÖ Perfect precision (100%) across all classes
- ‚úÖ Excellent recall (97.85% average)
- ‚úÖ Learns semantic patterns, not just keywords
- ‚úÖ Handles format variations well (e.g., "DELIVERY|ts=..." vs "{\"lvl\":\"info\"...")
- ‚úÖ Robust to training data size (works well with 438 training samples)
- ‚úÖ Perfect classification for database and authentication events (100% precision/recall)

**Weaknesses:**
- ‚ùå Slower training (requires neural network training epochs)
- ‚ùå More memory intensive
- ‚ùå Less interpretable than rule-based
- ‚ùå Minor confusion: 1 unknown ‚Üí None, 1 unknown ‚Üí error, 2 http_request ‚Üí None

**Why it performs best:**
1. **Semantic understanding**: Unlike keyword matching, it learns that "db.connecting", "db.connected", and "database connection" are related
2. **Context awareness**: Understands that "http.request.received" and "http.request.completed" are both HTTP events
3. **Format flexibility**: Can handle different log formats (pipe-separated, JSON, etc.) because it learns patterns, not exact matches
4. **Neural network architecture**: The underlying neural network can capture complex relationships between words and event types
5. **Larger training set**: 438 samples (vs 127 previously) enables better pattern learning

**Performance breakdown:**
- **Perfect classification** for database events (100% precision/recall)
- **Perfect classification** for authentication events (100% precision/recall)
- **Excellent** for unknown events (100% precision, 97.85% recall)
- **Excellent** for HTTP requests (100% precision, 96.92% recall)

**Best for:** Production use where accuracy is critical. Recommended classifier for this application.

---

### 4. TF-IDF + KNN - 53.76% Accuracy

**How it works:**
- Converts templates to TF-IDF vectors (term frequency-inverse document frequency)
- Uses K-Nearest Neighbors (K=3) with cosine similarity
- Classifies based on majority vote of nearest neighbors

**Strengths:**
- ‚úÖ Simple and interpretable (can see which neighbors influenced the decision)
- ‚úÖ No training required (lazy learning)
- ‚úÖ Handles new classes easily

**Weaknesses:**
- ‚ùå **Class collapse**: Predicts only "unknown" for all test samples (100% recall for unknown, 0% for others)
- ‚ùå Poor feature representation: TF-IDF may not capture semantic relationships
- ‚ùå Sensitive to feature scaling and vocabulary size
- ‚ùå Computationally expensive for large datasets (must compute similarity to all training samples)
- ‚ùå Worst performing classifier (53.76% accuracy)

**Why it underperforms:**
1. **Class imbalance**: The "unknown" class dominates training data (93/173 test samples), so most nearest neighbors are "unknown"
2. **Feature sparsity**: TF-IDF vectors are sparse, making similarity calculations less meaningful
3. **Vocabulary mismatch**: Test templates may have words not seen in training, leading to zero similarity
4. **K value**: K=3 may be too small for this dataset, causing it to default to the majority class
5. **No improvement with larger dataset**: Performance remains at 53.76% even with 438 training samples

**Potential improvements:**
- Increase K value (try K=5 or K=7)
- Use better feature engineering (n-grams, character-level features)
- Balance training data across classes
- Use different distance metrics (e.g., Jaccard similarity)
- Consider class weighting to reduce bias toward majority class

**Best for:** Balanced datasets with rich vocabulary overlap between train and test sets. Not recommended for this use case.

---

## Key Insights

### Why ML Approaches Can Outperform Rule-based

1. **Pattern Learning**: ML models learn patterns from data rather than relying on hardcoded rules
2. **Format Flexibility**: They handle variations in log formats better than keyword matching
3. **Context Understanding**: NLP-based models understand semantic relationships between words
4. **Adaptability**: Can improve with more training data without code changes
5. **Semantic Understanding**: NLP models can distinguish between similar patterns (e.g., "auth.login.request" in HTTP context vs authentication context)

### Why Some ML Models Underperform

1. **Naive Bayes**: Feature independence assumption violated, struggles with contextual dependencies
2. **TF-IDF KNN**: Class imbalance and feature sparsity issues cause class collapse
3. **Training data quality**: While improved (438 samples), class imbalance still affects performance
4. **Class distribution**: Imbalanced classes (93/173 "unknown" in test set) bias predictions

### Recommendations

1. **For Production**: Use **NLP-based** classifier (97.69% accuracy) - best overall performance
2. **For Development/Testing**: Use **Rule-based** (fast, interpretable, 93.06% accuracy) - strong baseline
3. **For Research**: Experiment with **Naive Bayes** improvements or **TF-IDF KNN** with better feature engineering

## Example Config

```json
{
  "classifier": "nlp-based",
  "options": {
    "k": 3
  }
}
```

## Summary

### Performance Ranking

1. **ü•á NLP-based (97.69%)** - Best overall performance, recommended for production
2. **ü•à Rule-based (93.06%)** - Strong baseline, good for development/testing
3. **ü•â Naive Bayes (81.50%)** - Good performance, but below NLP-based
4. **TF-IDF KNN (53.76%)** - Underperforms due to class collapse

### Key Takeaways

- **NLP-based classifier achieves 4.6% accuracy improvement** over the rule-based baseline (97.69% vs 93.06%)
- **Rule-based provides a strong baseline** (93.06%) - not trivial to beat, making ML improvements meaningful
- **Training data quality matters**: Larger dataset (438 samples) improved Naive Bayes from 34.55% to 81.50%
- **Class imbalance** significantly impacts model performance (especially TF-IDF KNN with class collapse)
- **Semantic understanding** (NLP) outperforms statistical methods (Naive Bayes, TF-IDF) for this task
- **Consistent evaluation methodology**: Using shared utilities ensures fair comparison and production simulation

### Next Steps for Improvement

1. **Expand training dataset**: Collect more diverse log examples (target: 1000+ samples)
2. **Balance classes**: Ensure all event types have sufficient training examples, especially "error" and "warning"
3. **Feature engineering**: For TF-IDF KNN, try n-grams and character-level features
4. **Hyperparameter tuning**: Experiment with different K values for KNN, regularization for Naive Bayes
5. **Ensemble methods**: Combine multiple classifiers for even better performance
6. **Error analysis**: Investigate the 4.6% gap between NLP-based and rule-based to identify improvement opportunities

